# -*- coding: utf-8 -*-
"""LSTM_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pja1UqWlYXCqgPEwuqLqdFSYMdwr0_64
"""

from google.colab import files

uploaded = files.upload()

import re
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
import keras
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
import math
import nltk

pip install pad_sequences

import pandas as pd
import io

headlines_data = pd.read_csv(io.BytesIO(uploaded['labeledcountriesdata.csv']))
del headlines_data['developed_exists']
del headlines_data['developing_exists']
del headlines_data['cat']
del headlines_data['tokenized_sents']
headlines_data

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
headlines_data['headline_text'] = headlines_data['headline_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))

nltk.download('wordnet')

w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
lemmatizer = nltk.stem.WordNetLemmatizer()
def lemmatize_text(text):
    st = ""
    for w in w_tokenizer.tokenize(text):
        st = st + lemmatizer.lemmatize(w) + " "
    return st
headlines_data['headline_text'] = headlines_data['headline_text'].apply(lemmatize_text)
headlines_data

from sklearn.preprocessing import LabelEncoder
headlines = headlines_data['headline_text'].values
labels = headlines_data['label'].values
encoder = LabelEncoder()
encoded_labels = encoder.fit_transform(labels)

train_sentences, test_sentences, train_labels, test_labels = train_test_split(headlines, encoded_labels, stratify = encoded_labels)

from keras.preprocessing.text import Tokenizer
vocab_size = 1988 # choose based on statistics
oov_tok = ''
embedding_dim = 100
max_length = 79 # choose based on statistics, for example 150 to 200
padding_type='post'
trunc_type='post'
# tokenize sentences
tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(train_sentences)
word_index = tokenizer.word_index
# convert train dataset to sequence and pad sequences
train_sequences = tokenizer.texts_to_sequences(train_sentences)
train_padded = pad_sequences(train_sequences, padding='post', maxlen=max_length)
# convert Test dataset to sequence and pad sequences
test_sequences = tokenizer.texts_to_sequences(test_sentences)
test_padded = pad_sequences(test_sequences, padding='post', maxlen=max_length)

pip install tensorflow

model = keras.Sequential([
    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    keras.layers.Bidirectional(keras.layers.LSTM(64)),
    keras.layers.Dense(24, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')
])
# compile model
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
# model summary
model.summary()

num_epochs = 12
history = model.fit(train_padded, train_labels,
                    epochs=num_epochs, verbose=1,
                    validation_split=0.1)

prediction = model.predict(test_padded)
# Get labels based on probability 1 if p>= 0.5 else 0
pred_labels = []
for i in prediction:
    if i >= 0.5:
        pred_labels.append(1)
    else:
      pred_labels.append(0)
print("Accuracy of prediction on test set : ", accuracy_score(test_labels,pred_labels))

data = files.upload()

countries_data = pd.read_csv(io.BytesIO(data['countries_headlines-3 - Sheet1.csv']))

# convert to a sequence
sequences = tokenizer.texts_to_sequences(countries_data['headline_text'])
# pad the sequence
padded = pad_sequences(sequences, padding='post', maxlen=max_length)
# Get labels based on probability 1 if p>= 0.5 else 0
prediction = model.predict(padded)
model.evaluate(test_padded, y_test) # where y_test will be the dummies for original y
pred_labels = []
for i in prediction:
    if i >= 0.5:
        pred_labels.append(1)
    else:
        pred_labels.append(0)
countries_data['label'] = pred_labels
countries_data